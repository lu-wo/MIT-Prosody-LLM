{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, AutoModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer_fast = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "gpt2_model = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love my dog, because he is brave.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love my dog, because he is brave.\"\n",
    "# text = \"[SOS] \" + text + \" [EOS]\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = gpt2_tokenizer.encode(text, add_special_tokens=False)\n",
    "encoding_fast = gpt2_tokenizer_fast.encode(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1842, 616, 3290, 11, 780, 339, 318, 14802, 13]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1842, 616, 3290, 11, 780, 339, 318, 14802, 13]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I love my dog, because he is brave.', 'I love my dog, because he is brave.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding = gpt2_tokenizer.decode(encoding)\n",
    "decoding_fast = gpt2_tokenizer_fast.decode(encoding_fast)\n",
    "decoding, decoding_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1212,   318,   281,  1672,  6827,    13],\n",
      "        [ 6610,  1672,  6827,    13, 50256, 50256],\n",
      "        [   32,  1790,   530,    13, 50256, 50256]])\n",
      "Attention masks: tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This is an example sentence.', 'Another example sentence.', 'A short one.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence (EOS) token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "]\n",
    "\n",
    "# Use the tokenizer to encode and pad the sentences\n",
    "encoded_batch = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    padding=True,  # Enables padding\n",
    "    return_tensors=\"pt\",  # Returns PyTorch tensors (use \"tf\" for TensorFlow tensors)\n",
    ")\n",
    "\n",
    "# Access the padded input IDs and attention masks\n",
    "input_ids = encoded_batch[\"input_ids\"]\n",
    "attention_masks = encoded_batch[\"attention_mask\"]\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention masks:\", attention_masks)\n",
    "\n",
    "# batch decode where attention_mask is used to ignore padding tokens\n",
    "tokenizer.batch_decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert tokenizer and fast\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "bert_tokenizer_fast = BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love my dog, because he is brave.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love my dog, because he is brave.\"\n",
    "# text = \"[SOS] \" + text + \" [EOS]\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "encodings_fast = bert_tokenizer_fast.encode(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2023, 2003, 2019, 2742, 6251, 1012,  102],\n",
      "        [ 101, 2178, 2742, 6251, 1012,  102,    0,    0],\n",
      "        [ 101, 1037, 2460, 2028, 1012,  102,    0,    0]])\n",
      "Attention masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "['this is an example sentence.', 'another example sentence.', 'a short one.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "]\n",
    "\n",
    "# Use the tokenizer to encode and pad the sentences\n",
    "encoded_batch = tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    padding=True,  # Enables padding\n",
    "    return_tensors=\"pt\",  # Returns PyTorch tensors (use \"tf\" for TensorFlow tensors)\n",
    ")\n",
    "\n",
    "# Access the padded input IDs and attention masks\n",
    "input_ids = encoded_batch[\"input_ids\"]\n",
    "attention_masks = encoded_batch[\"attention_mask\"]\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention masks:\", attention_masks)\n",
    "\n",
    "# decoding\n",
    "decoding = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "]\n",
    "\n",
    "labels = [[1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.components.datasets import encode_and_pad_batch, TokenTaggingDataset\n",
    "\n",
    "dataset = TokenTaggingDataset(sentences, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: encode_and_pad_batch(batch, tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text \n",
      " This is an example sentence.\n",
      "word encodings \n",
      " [[2023], [2003], [2019], [2742], [6251, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2], [3], [4, 5]]\n",
      "#tokens \n",
      " 6\n",
      "algined tokens \n",
      " [2023, 2003, 2019, 2742, 6251, 1012]\n",
      "aligned decoded\n",
      " this is an example sentence.\n",
      "text \n",
      " Another example sentence.\n",
      "word encodings \n",
      " [[2178], [2742], [6251, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2, 3]]\n",
      "#tokens \n",
      " 4\n",
      "algined tokens \n",
      " [2178, 2742, 6251, 1012]\n",
      "aligned decoded\n",
      " another example sentence.\n",
      "Input IDs:\n",
      " tensor([[ 101, 2023, 2003, 2019, 2742, 6251, 1012,  102],\n",
      "        [ 101, 2178, 2742, 6251, 1012,  102,    0,    0]])\n",
      "Decoded input:\n",
      " ['[CLS] this is an example sentence. [SEP]', '[CLS] another example sentence. [SEP] [PAD] [PAD]']\n",
      "Attention masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "Padded labels:\n",
      " tensor([[   1,    1,    1,    1,    1,    1, -999, -999],\n",
      "        [   1,    1,    1,    1, -999, -999, -999, -999]])\n",
      "Outputs:\n",
      " tensor([[[-0.3774, -0.3350, -0.3206,  ..., -0.5255,  0.2590,  0.6877],\n",
      "         [-0.8629, -0.6322, -0.4241,  ..., -0.5824,  0.7432,  0.1259],\n",
      "         [-0.2213, -0.9393,  0.3523,  ..., -0.3219,  0.4667,  0.5915],\n",
      "         ...,\n",
      "         [-0.0471, -0.1261,  0.0237,  ..., -0.2642,  0.1113, -0.0666],\n",
      "         [ 0.7917,  0.0632, -0.6609,  ...,  0.3360, -0.7243, -0.2557],\n",
      "         [-0.1416, -0.3367, -0.3314,  ..., -0.2132, -0.4376,  0.4454]],\n",
      "\n",
      "        [[-0.2924, -0.3253, -0.3886,  ..., -0.2868,  0.2157,  0.6433],\n",
      "         [-0.0338, -1.0024, -0.3005,  ..., -0.1190,  0.9047,  0.2311],\n",
      "         [-0.4415, -0.2425, -0.7587,  ..., -0.6208,  0.1528,  0.0825],\n",
      "         ...,\n",
      "         [ 1.0391, -0.0324, -0.4098,  ...,  0.2882, -0.8480, -0.1672],\n",
      "         [-0.3579, -0.4596, -0.1465,  ..., -0.0685,  0.0276,  0.2554],\n",
      "         [-0.3476, -0.5998, -0.2705,  ...,  0.0828,  0.1289,  0.2535]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Outputs shape:\n",
      " torch.Size([2, 8, 768])\n",
      "text \n",
      " A short one.\n",
      "word encodings \n",
      " [[1037], [2460], [2028, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2, 3]]\n",
      "#tokens \n",
      " 4\n",
      "algined tokens \n",
      " [1037, 2460, 2028, 1012]\n",
      "aligned decoded\n",
      " a short one.\n",
      "text \n",
      " What a long sentence this here is incredible.\n",
      "word encodings \n",
      " [[2054], [1037], [2146], [6251], [2023], [2182], [2003], [9788, 1012]]\n",
      "word_to_token\n",
      " [[0], [1], [2], [3], [4], [5], [6], [7, 8]]\n",
      "#tokens \n",
      " 9\n",
      "algined tokens \n",
      " [2054, 1037, 2146, 6251, 2023, 2182, 2003, 9788, 1012]\n",
      "aligned decoded\n",
      " what a long sentence this here is incredible.\n",
      "Input IDs:\n",
      " tensor([[ 101, 1037, 2460, 2028, 1012,  102,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2054, 1037, 2146, 6251, 2023, 2182, 2003, 9788, 1012,  102]])\n",
      "Decoded input:\n",
      " ['[CLS] a short one. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] what a long sentence this here is incredible. [SEP]']\n",
      "Attention masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Padded labels:\n",
      " tensor([[   1,    1,    1,    1, -999, -999, -999, -999, -999, -999, -999],\n",
      "        [   1,    1,    1,    1,    1,    1,    1,    1,    1, -999, -999]])\n",
      "Outputs:\n",
      " tensor([[[-0.1449, -0.5376, -0.7223,  ..., -0.1883,  0.2320,  0.4892],\n",
      "         [ 0.4290, -0.1785, -0.9902,  ..., -0.9439,  0.1389,  0.7930],\n",
      "         [ 0.2519,  0.0264, -0.3542,  ..., -0.6387,  0.0040, -0.1435],\n",
      "         ...,\n",
      "         [ 0.1282, -0.2647, -0.0093,  ..., -0.0088,  0.0632,  0.0632],\n",
      "         [-0.2388, -0.8124, -0.1797,  ...,  0.4322,  0.2350, -0.1776],\n",
      "         [-0.1423, -0.8497, -0.1566,  ...,  0.3646,  0.2033, -0.1145]],\n",
      "\n",
      "        [[-0.0040, -0.0363, -0.0509,  ..., -0.1111,  0.2380,  0.5907],\n",
      "         [ 0.6698,  0.7413,  0.1565,  ..., -0.2265,  0.4102,  0.3011],\n",
      "         [ 0.0129,  0.4222,  0.5661,  ...,  0.6035,  0.6153,  0.5633],\n",
      "         ...,\n",
      "         [ 0.6282,  0.7849, -0.0963,  ...,  0.2277,  0.4458,  0.1739],\n",
      "         [ 0.3752,  0.0725, -0.3378,  ...,  0.1718, -0.3127, -0.5080],\n",
      "         [ 0.6674,  0.0567, -0.1778,  ...,  0.0407, -0.5813, -0.4047]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Outputs shape:\n",
      " torch.Size([2, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, BertTokenizer, AutoTokenizer, AutoModel\n",
    "from src.data.components.datasets import encode_and_pad_batch, TokenTaggingDataset\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "\n",
    "# distribute_word_label_to_token function from a previous response\n",
    "\n",
    "# encode_and_pad_batch function from your message\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, add_prefix_space=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "cfg = DictConfig({\"model\": \"bert-base-uncased\"})\n",
    "\n",
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"A short one.\",\n",
    "    \"What a long sentence this here is incredible.\",\n",
    "]\n",
    "labels = [[1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]\n",
    "\n",
    "dataset = TokenTaggingDataset(sentences, labels, tokenizer, cfg)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=lambda batch: encode_and_pad_batch(batch, tokenizer),\n",
    ")\n",
    "\n",
    "for input_ids, attention_masks, padded_labels in dataloader:\n",
    "    print(\"Input IDs:\\n\", input_ids)\n",
    "    print(\n",
    "        \"Decoded input:\\n\", tokenizer.batch_decode(input_ids, skip_special_tokens=False)\n",
    "    )\n",
    "    print(\"Attention masks:\\n\", attention_masks)\n",
    "    print(\"Padded labels:\\n\", padded_labels)\n",
    "    outputs = model(input_ids, attention_mask=attention_masks).last_hidden_state\n",
    "    print(\"Outputs:\\n\", outputs)\n",
    "    print(\"Outputs shape:\\n\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
